# Kartostack

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Ollama](https://img.shields.io/badge/Ollama-v0.14.3+-blue.svg)](https://ollama.com)
[![Docker](https://img.shields.io/badge/Docker-Required-blue.svg)](https://www.docker.com/)
[![GLM-4.7](https://img.shields.io/badge/Model-GLM--4.7--Flash-green.svg)](https://ollama.com/library/glm-4.7-flash)
[![Claude Code](https://img.shields.io/badge/Claude%20Code-CLI-purple.svg)](https://claude.ai)

**Asistente de programaciÃ³n autÃ³nomo 24/7** containerizado con Claude Code y modelos locales via Ollama.

> **GuÃ­a completa**: Lee el artÃ­culo [Agente de Coding 24/7 prÃ¡cticamente gratis](https://karim.touma.io/agente-de-ingenier%C3%ADa-de-software-24-7-pr%C3%A1cticamente-gratis-4cbd12e2bd92) para entender la arquitectura completa, casos de uso y filosofÃ­a detrÃ¡s del proyecto.

---

## Â¿QuÃ© es Kartostack?

Kartostack es un entorno containerizado que combina:

- **Claude Code CLI** - El agente de programaciÃ³n de Anthropic
- **GLM-4.7-Flash** - Modelo de lenguaje local con 82k tokens de contexto
- **Ollama** - Servidor de inferencia local con aceleraciÃ³n GPU
- **MCP Servers** - Herramientas para bÃºsqueda web, browser automation y filesystem
- **Ralph Wiggum** - Plugin para tareas de larga duraciÃ³n con loops iterativos

El resultado: un agente de programaciÃ³n que puede trabajar **horas sin supervisiÃ³n**, con acceso a internet, automatizaciÃ³n de browser y gestiÃ³n inteligente de memoria.

---

## Stack TecnolÃ³gico

```
VS Code + Claude Code + GLM-4.7-Flash + Ollama + Serper MCP + Puppeteer MCP + Ralph-Wiggum
```

| Componente | FunciÃ³n | UbicaciÃ³n |
|------------|---------|-----------|
| **Claude Code** | Agente CLI de programaciÃ³n | Container |
| **GLM-4.7-Flash** | LLM (82k contexto) | Host (Ollama) |
| **Ollama** | Servidor de inferencia | Host (nativo) |
| **Serper MCP** | BÃºsqueda web (Google) | Container |
| **Puppeteer MCP** | Browser automation | Container |
| **Filesystem MCP** | Acceso a archivos | Container |
| **Ralph Wiggum** | Loops iterativos | Container |

---

## CaracterÃ­sticas Principales

### ğŸ¤– LLM Local con Contexto Amplio
- Modelo GLM-4.7-Flash corriendo localmente via Ollama
- **82,000 tokens de contexto** (configurable hasta 198k)
- AceleraciÃ³n GPU con Apple Metal (macOS) o CUDA (Linux)
- Sin lÃ­mites de API ni costos por token

### ğŸŒ BÃºsqueda Web Integrada
- BÃºsqueda en Google via Serper API
- BÃºsqueda de noticias y imÃ¡genes
- Web scraping con extracciÃ³n de texto limpio
- El agente puede investigar antes de implementar

### ğŸ–¥ï¸ Browser Automation
- Chromium headless integrado
- NavegaciÃ³n, clicks, formularios
- Screenshots y evaluaciÃ³n de JavaScript
- Ideal para testing y scraping de SPAs

### ğŸ“ Acceso Completo al Sistema de Archivos
- Lectura y escritura de archivos
- NavegaciÃ³n de directorios
- El agente puede modificar tu cÃ³digo directamente

### ğŸ”„ Tareas de Larga DuraciÃ³n (Ralph Wiggum)
- Loops iterativos con condiciones de salida
- CompresiÃ³n automÃ¡tica de memoria al 70% del contexto
- PreservaciÃ³n de trabajo en archivos y git
- Ideal para refactorings, migraciones, implementaciones complejas

### ğŸ“¦ Containerizado y Portable
- Todo empaquetado en Docker
- ConfiguraciÃ³n persistente en volumen
- FÃ¡cil de replicar en cualquier mÃ¡quina

---

## Requisitos del Sistema

### Hardware Recomendado

| Componente | MÃ­nimo | Recomendado |
|------------|--------|-------------|
| **RAM** | 32GB | 64GB+ |
| **VRAM/Unified** | 32GB | 64GB+ |
| **Disco** | 50GB libres | 100GB libres |
| **GPU** | Apple M1/M2 | Apple M3/M4 Max |

> **Nota sobre memoria**: GLM-4.7-Flash con 82k de contexto usa ~97GB de memoria (modelo q8_0 + KV cache). En Macs con memoria unificada, esto funciona bien con 128GB.

### Software Requerido

- **macOS** con Apple Silicon (M1/M2/M3/M4) para aceleraciÃ³n GPU
  - TambiÃ©n funciona en Linux con CUDA
- **Docker Desktop** v4.0+
- **Ollama v0.14.3-rc3+** (requerido para GLM-4.7-flash)
  - âš ï¸ La versiÃ³n estable 0.14.2 NO soporta este modelo

### Opcional

- **Serper API Key** - Para bÃºsquedas web ([serper.dev](https://serper.dev))
- **VS Code** - Para integraciÃ³n con Claude Code extension

---

## InstalaciÃ³n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/karimtouma/kartostack.git
cd kartostack
```

### 2. Hacer Scripts Ejecutables

```bash
chmod +x scripts/*.sh
chmod +x entrypoint.sh
chmod +x plugins/ralph-wiggum/hooks/*.sh
chmod +x plugins/ralph-wiggum/scripts/*.sh
```

### 3. Instalar Ollama RC + Modelo

El script detecta tu sistema operativo y arquitectura automÃ¡ticamente:

```bash
# Solo instalar Ollama RC
./scripts/install-ollama.sh

# Instalar Ollama RC + descargar modelo + crear variante 82k
./scripts/install-ollama.sh --with-model
```

**Â¿QuÃ© hace el script?**
1. Detecta si es macOS o Linux
2. Descarga Ollama v0.14.3-rc3 desde GitHub releases
3. Instala en `/Applications` (macOS) o `/usr/local/bin` (Linux)
4. Opcionalmente descarga `glm-4.7-flash` (~19-32GB)
5. Crea la variante `glm-4.7-flash-82k` con contexto expandido

### 4. Configurar Entorno

```bash
cp .env.example .env
```

Edita `.env` con tu configuraciÃ³n:

```bash
# Modelo a usar
CLAUDE_MODEL=glm-4.7-flash-82k

# Tu directorio de proyectos
WORKSPACE=/Users/tu-usuario/Projects

# API key de Serper (opcional, para bÃºsqueda web)
SERPER_API_KEY=tu_api_key_aqui
```

### 5. Construir Imagen Docker

```bash
docker build -t kartostack .
```

### 6. Ejecutar

```bash
./scripts/run.sh
```

---

## Uso

### Modo Interactivo (Default)

```bash
./scripts/run.sh
```

Esto abre una sesiÃ³n interactiva donde puedes chatear con el agente.

### Modo No Interactivo

Ejecutar una tarea especÃ­fica y obtener el resultado:

```bash
# Tarea simple
./scripts/run.sh --print -p "Crea un script hello world en Python"

# Tarea con contexto
./scripts/run.sh --print -p "Analiza el cÃ³digo en src/ y genera un reporte de arquitectura"
```

### Tareas de Larga DuraciÃ³n (Ralph Wiggum)

Para tareas complejas que requieren mÃºltiples iteraciones:

```bash
./scripts/run.sh
```

Dentro del agente:

```
/ralph-loop "Implementa autenticaciÃ³n JWT en el proyecto" --max-iterations 30 --completion-promise "AUTH COMPLETE"
```

**ParÃ¡metros:**
- `--max-iterations N` - MÃ¡ximo de iteraciones (default: 50)
- `--completion-promise "TEXT"` - Texto que seÃ±ala completado

**Para seÃ±alar que terminaste:**
```
<promise>AUTH COMPLETE</promise>
```

### Ejemplos de Prompts Efectivos

#### InvestigaciÃ³n + ImplementaciÃ³n
```
Usa web_search para encontrar la documentaciÃ³n oficial de FastAPI sobre
dependency injection. Luego implementa un sistema de autenticaciÃ³n
basado en esos patrones en src/auth/. Criterio de completitud:
todos los tests en tests/auth/ deben pasar.
```

#### Refactoring con Tests
```
Refactoriza el mÃ³dulo src/database/ para usar el patrÃ³n Repository.
Criterios:
- Mantener 100% de compatibilidad con la API existente
- Agregar tests unitarios para cada repository
- pnpm test debe pasar sin errores
- Genera REFACTOR_REPORT.md con los cambios realizados
```

#### Scraping con Browser
```
Usa puppeteer_navigate para ir a https://example.com/pricing.
Extrae la tabla de precios y guÃ¡rdala como JSON en data/pricing.json.
Toma un screenshot y guÃ¡rdalo en screenshots/pricing.png.
```

---

## Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DOCKER CONTAINER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                      CLAUDE CODE CLI                          â”‚  â”‚
â”‚   â”‚                                                                â”‚  â”‚
â”‚   â”‚   â€¢ Agente de programaciÃ³n autÃ³nomo                          â”‚  â”‚
â”‚   â”‚   â€¢ Lee/escribe archivos, ejecuta comandos                   â”‚  â”‚
â”‚   â”‚   â€¢ Protocolo MCP para herramientas externas                 â”‚  â”‚
â”‚   â”‚   â€¢ GestiÃ³n de contexto y memoria                            â”‚  â”‚
â”‚   â”‚                                                                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                       â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚               â”‚               â”‚                      â”‚
â”‚              â–¼               â–¼               â–¼                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚  SERPER MCP  â”‚  â”‚FILESYSTEM MCPâ”‚  â”‚PUPPETEER MCP â”‚             â”‚
â”‚   â”‚              â”‚  â”‚              â”‚  â”‚              â”‚             â”‚
â”‚   â”‚ â€¢ web_search â”‚  â”‚ â€¢ read_file  â”‚  â”‚ â€¢ navigate   â”‚             â”‚
â”‚   â”‚ â€¢ news_searchâ”‚  â”‚ â€¢ write_file â”‚  â”‚ â€¢ click      â”‚             â”‚
â”‚   â”‚ â€¢ scrape_url â”‚  â”‚ â€¢ list_dir   â”‚  â”‚ â€¢ fill       â”‚             â”‚
â”‚   â”‚ â€¢ fetch_url  â”‚  â”‚              â”‚  â”‚ â€¢ screenshot â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                    RALPH WIGGUM PLUGIN                        â”‚  â”‚
â”‚   â”‚                                                                â”‚  â”‚
â”‚   â”‚   â€¢ /ralph-loop - Iniciar loop iterativo                     â”‚  â”‚
â”‚   â”‚   â€¢ /cancel-ralph - Cancelar loop activo                     â”‚  â”‚
â”‚   â”‚   â€¢ Auto-compresiÃ³n de memoria al 70% del contexto           â”‚  â”‚
â”‚   â”‚   â€¢ PreservaciÃ³n de estado en archivos                       â”‚  â”‚
â”‚   â”‚                                                                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â”‚ HTTP (Anthropic API)
                                   â”‚ localhost:11434
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HOST (macOS/Linux)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                         OLLAMA                                â”‚  â”‚
â”‚   â”‚                                                                â”‚  â”‚
â”‚   â”‚   â€¢ Servidor de inferencia local                             â”‚  â”‚
â”‚   â”‚   â€¢ API compatible con Anthropic Messages                    â”‚  â”‚
â”‚   â”‚   â€¢ AceleraciÃ³n GPU (Metal/CUDA)                             â”‚  â”‚
â”‚   â”‚                                                                â”‚  â”‚
â”‚   â”‚   Modelo: GLM-4.7-Flash                                      â”‚  â”‚
â”‚   â”‚   Contexto: 82,000 tokens                                    â”‚  â”‚
â”‚   â”‚   Memoria: ~97GB (q8_0 + KV cache)                           â”‚  â”‚
â”‚   â”‚                                                                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Â¿Por quÃ© Ollama corre en el Host y no en Docker?

**Docker en macOS NO puede acceder a la GPU Metal**. Por lo tanto:
- Ollama debe correr **nativo** para usar aceleraciÃ³n GPU (5-6x mÃ¡s rÃ¡pido)
- El container se conecta al host via `host.docker.internal:11434`
- Esto es transparente para el usuario

---

## ConfiguraciÃ³n Avanzada

### Variables de Entorno

| Variable | Default | DescripciÃ³n |
|----------|---------|-------------|
| `CLAUDE_MODEL` | `glm-4.7-flash-82k` | Modelo de Ollama a usar |
| `OLLAMA_HOST` | `localhost` | Host del servidor Ollama |
| `OLLAMA_PORT` | `11434` | Puerto del servidor Ollama |
| `WORKSPACE` | `$HOME/Projects` | Directorio montado como /workspace |
| `IMAGE_NAME` | `kartostack` | Nombre de la imagen Docker |
| `VOLUME_NAME` | `kartostack-data` | Nombre del volumen para persistencia |
| `SERPER_API_KEY` | - | API key de Serper para bÃºsqueda web |

### Variantes del Modelo

El modelo base `glm-4.7-flash` tiene 198k de contexto mÃ¡ximo. Puedes crear variantes con diferentes tamaÃ±os:

```bash
# Variante rÃ¡pida (32k contexto, menos memoria)
cat > /tmp/Modelfile.32k << 'EOF'
FROM glm-4.7-flash
PARAMETER num_ctx 32768
EOF
ollama create glm-4.7-flash-32k -f /tmp/Modelfile.32k

# Variante mÃ¡xima (198k contexto, mucha memoria)
cat > /tmp/Modelfile.198k << 'EOF'
FROM glm-4.7-flash
PARAMETER num_ctx 198000
EOF
ollama create glm-4.7-flash-198k -f /tmp/Modelfile.198k
```

| Variante | Contexto | Memoria Aprox. | Uso |
|----------|----------|----------------|-----|
| 32k | 32,768 | ~40GB | Tareas rÃ¡pidas |
| 82k | 82,000 | ~97GB | **Default** - Balance |
| 198k | 198,000 | ~200GB | Tareas con mucho contexto |

### Mantener Modelo en Memoria 24/7

Para que Ollama no descargue el modelo despuÃ©s de un tiempo de inactividad:

```bash
# Configurar keep_alive infinito
ollama run glm-4.7-flash-82k --keepalive -1
```

O agregar a tu Modelfile:
```
PARAMETER keep_alive -1
```

### ConfiguraciÃ³n de MCP Servers

Los servidores MCP estÃ¡n configurados en `mcp-config.json`:

```json
{
  "mcpServers": {
    "serper": {
      "command": "node",
      "args": ["/app/mcp-servers/serper/index.js"],
      "env": { "SERPER_API_KEY": "${SERPER_API_KEY}" }
    },
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/workspace"]
    },
    "puppeteer": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-puppeteer"],
      "env": { "PUPPETEER_EXECUTABLE_PATH": "/usr/bin/chromium" }
    }
  }
}
```

---

## Herramientas MCP Disponibles

### Serper (BÃºsqueda Web)

| Herramienta | DescripciÃ³n | Ejemplo |
|-------------|-------------|---------|
| `web_search` | BÃºsqueda en Google | `web_search("FastAPI authentication")` |
| `news_search` | BÃºsqueda de noticias | `news_search("OpenAI GPT-5")` |
| `image_search` | BÃºsqueda de imÃ¡genes | `image_search("React architecture diagram")` |
| `scrape_webpage` | Scraping completo | `scrape_webpage("https://docs.python.org")` |
| `fetch_url` | ExtracciÃ³n de texto limpio | `fetch_url("https://example.com/article")` |

### Puppeteer (Browser Automation)

| Herramienta | DescripciÃ³n |
|-------------|-------------|
| `puppeteer_navigate` | Navegar a URL |
| `puppeteer_screenshot` | Capturar screenshot |
| `puppeteer_click` | Click en elemento |
| `puppeteer_fill` | Llenar input/textarea |
| `puppeteer_select` | Seleccionar en dropdown |
| `puppeteer_hover` | Hover sobre elemento |
| `puppeteer_evaluate` | Ejecutar JavaScript |

### Filesystem

| Herramienta | DescripciÃ³n |
|-------------|-------------|
| `read_file` | Leer contenido de archivo |
| `write_file` | Escribir/crear archivo |
| `list_directory` | Listar contenido de directorio |

---

## Plugin Ralph Wiggum

Ralph Wiggum implementa la tÃ©cnica de **loops iterativos con condiciones de salida** para tareas de larga duraciÃ³n.

### Comandos Disponibles

| Comando | DescripciÃ³n |
|---------|-------------|
| `/ralph-loop` | Iniciar loop iterativo |
| `/cancel-ralph` | Cancelar loop activo |
| `/help` | Ayuda del plugin |

### CÃ³mo Funciona

1. Defines una tarea con criterios de completitud
2. El agente trabaja iterativamente
3. En cada iteraciÃ³n, evalÃºa si cumpliÃ³ los criterios
4. Si los cumple, imprime `<promise>CRITERIO</promise>`
5. El loop termina cuando se cumple o se alcanza max iterations

### Auto-CompresiÃ³n de Memoria

Cuando el contexto alcanza ~70% del lÃ­mite (70k tokens para modelo 82k):

1. El hook `stop-hook.sh` detecta el tamaÃ±o
2. Guarda contexto crÃ­tico en `.claude/memory.md`
3. Inicia nueva sesiÃ³n con instrucciones de leer el archivo
4. El agente continÃºa donde quedÃ³

Esto permite tareas que duran **horas** sin perder contexto.

---

## SoluciÃ³n de Problemas

### Ollama no responde

```bash
# Verificar si Ollama estÃ¡ corriendo
curl http://localhost:11434/api/tags

# Ver logs de Ollama
tail -f ~/.ollama/logs/server.log

# Reiniciar Ollama
pkill ollama
open -a Ollama  # macOS
# o: systemctl restart ollama  # Linux
```

### Modelo no cargado en memoria

```bash
# Ver modelos cargados actualmente
ollama ps

# Pre-cargar el modelo
ollama run glm-4.7-flash-82k ""

# Verificar uso de memoria
# macOS:
vm_stat | grep "Pages"
# Linux:
free -h
```

### Error de permisos en Docker

```bash
# Resetear volumen (pierde configuraciÃ³n persistida)
docker volume rm kartostack-data

# Reconstruir imagen
docker build --no-cache -t kartostack .
```

### VersiÃ³n de Ollama incorrecta

```bash
# Verificar versiÃ³n actual
ollama --version

# Si es menor a 0.14.3, reinstalar
./scripts/install-ollama.sh
```

### Serper MCP no funciona

```bash
# Verificar que la API key estÃ¡ configurada
echo $SERPER_API_KEY

# Probar la API directamente
curl -X POST 'https://google.serper.dev/search' \
  -H 'X-API-KEY: TU_API_KEY' \
  -H 'Content-Type: application/json' \
  -d '{"q": "test"}'
```

### Puppeteer no puede iniciar Chromium

```bash
# Verificar que Chromium estÃ¡ instalado en el container
docker run --rm kartostack which chromium

# Si falla, reconstruir imagen
docker build --no-cache -t kartostack .
```

### El agente se queda "pensando" mucho tiempo

- GLM-4.7-Flash es mÃ¡s lento que modelos cloud
- En Apple Silicon M4 Max: ~60 tokens/segundo
- Prompts largos pueden tardar minutos
- Considera usar la variante 32k para tareas rÃ¡pidas

---

## Estructura del Proyecto

```
kartostack/
â”œâ”€â”€ .env.example              # Template de configuraciÃ³n
â”œâ”€â”€ .gitignore                # Archivos ignorados por git
â”œâ”€â”€ Dockerfile                # Imagen del container
â”œâ”€â”€ entrypoint.sh             # Script de entrada del container
â”œâ”€â”€ Modelfile.82k             # DefiniciÃ³n del modelo con 82k contexto
â”œâ”€â”€ README.md                 # Esta documentaciÃ³n
â”œâ”€â”€ CLAUDE.md                 # Instrucciones para el modelo
â”œâ”€â”€ claude.json               # Estado pre-configurado de Claude Code
â”œâ”€â”€ mcp-config.json           # ConfiguraciÃ³n de servidores MCP
â”œâ”€â”€ settings.json             # ConfiguraciÃ³n de Claude Code
â”‚
â”œâ”€â”€ mcp-servers/
â”‚   â””â”€â”€ serper/
â”‚       â”œâ”€â”€ index.js          # Servidor MCP de Serper
â”‚       â””â”€â”€ package.json      # Dependencias
â”‚
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ ralph-wiggum/
â”‚       â”œâ”€â”€ .claude-plugin/
â”‚       â”‚   â””â”€â”€ plugin.json   # Metadata del plugin
â”‚       â”œâ”€â”€ commands/
â”‚       â”‚   â”œâ”€â”€ ralph-loop.md # Comando principal
â”‚       â”‚   â”œâ”€â”€ cancel-ralph.md
â”‚       â”‚   â”œâ”€â”€ compress-memory.md
â”‚       â”‚   â””â”€â”€ help.md
â”‚       â”œâ”€â”€ hooks/
â”‚       â”‚   â”œâ”€â”€ hooks.json    # ConfiguraciÃ³n de hooks
â”‚       â”‚   â””â”€â”€ stop-hook.sh  # Hook de compresiÃ³n
â”‚       â”œâ”€â”€ scripts/
â”‚       â”‚   â”œâ”€â”€ setup-ralph-loop.sh
â”‚       â”‚   â””â”€â”€ compress-memory.sh
â”‚       â””â”€â”€ README.md
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ install-ollama.sh     # Instalador de Ollama RC
    â””â”€â”€ run.sh                # Script principal de ejecuciÃ³n
```

---

## Seguridad

### Consideraciones

- El agente tiene **acceso completo** al directorio montado como workspace
- Puede ejecutar comandos de sistema dentro del container
- Puede hacer requests HTTP a internet (si Serper estÃ¡ configurado)
- **No lo uses en directorios con informaciÃ³n sensible**

### Recomendaciones

1. **Monta solo lo necesario** - No montes `/` o `$HOME` completo
2. **Revisa los cambios** - Usa git para trackear cambios del agente
3. **Limita las iteraciones** - Siempre usa `--max-iterations` con Ralph
4. **Revisa el output** - Especialmente en tareas de larga duraciÃ³n

---

## Contribuir

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

---

## Referencias

### DocumentaciÃ³n Oficial
- [Claude Code Docs](https://code.claude.com/docs/en/overview)
- [Ollama Documentation](https://ollama.com/docs)
- [MCP Protocol Specification](https://spec.modelcontextprotocol.io/)

### ArtÃ­culos y GuÃ­as
- [Agente de Coding 24/7 prÃ¡cticamente gratis](https://karim.touma.io/agente-de-ingenier%C3%ADa-de-software-24-7-pr%C3%A1cticamente-gratis-4cbd12e2bd92) - GuÃ­a completa del autor
- [Ollama + Claude Code](https://ollama.com/blog/claude) - Blog oficial de Ollama
- [Historia de Ralph Wiggum](https://www.humanlayer.dev/blog/brief-history-of-ralph) - Origen de la tÃ©cnica

### Recursos
- [GLM-4.7-Flash en Ollama](https://ollama.com/library/glm-4.7-flash)
- [Serper API](https://serper.dev)
- [Puppeteer MCP Server](https://github.com/anthropics/anthropic-quickstarts)

---

## Autor

**Karim Touma**

- [Blog](https://karim.touma.io)
- [LinkedIn](https://www.linkedin.com/in/katouma/)
- [Twitter/X](https://x.com/karim_op)

---

## Licencia

MIT License - ver [LICENSE](LICENSE) para detalles.

---

<p align="center">
  <i>Hecho con â¤ï¸ y mucho cafÃ© por <a href="https://karim.touma.io">Karim Touma</a></i>
</p>
