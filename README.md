# Kartostack

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Ollama](https://img.shields.io/badge/Ollama-v0.14.3+-blue.svg)](https://ollama.com)
[![Docker](https://img.shields.io/badge/Docker-Required-blue.svg)](https://www.docker.com/)
[![GLM-4.7](https://img.shields.io/badge/Model-GLM--4.7--Flash-green.svg)](https://ollama.com/library/glm-4.7-flash)
[![Claude Code](https://img.shields.io/badge/Claude%20Code-CLI-purple.svg)](https://claude.ai)

**Asistente de programaci√≥n aut√≥nomo 24/7** containerizado con Claude Code y modelos locales via Ollama.

> **Gu√≠a completa**: Lee el art√≠culo [Agente de Coding 24/7 pr√°cticamente gratis](https://karim.touma.io/agente-de-ingenier%C3%ADa-de-software-24-7-pr%C3%A1cticamente-gratis-4cbd12e2bd92) para entender la arquitectura completa, casos de uso y filosof√≠a detr√°s del proyecto.

---

## ¬øQu√© es Kartostack?

Kartostack es un entorno containerizado que combina:

- **Claude Code CLI** - El agente de programaci√≥n de Anthropic
- **GLM-4.7-Flash** - Modelo de lenguaje local con 82k tokens de contexto
- **Ollama** - Servidor de inferencia local con aceleraci√≥n GPU
- **MCP Servers** - Herramientas para b√∫squeda web, browser automation y filesystem
- **Ralph Wiggum** - Plugin para tareas de larga duraci√≥n con loops iterativos

El resultado: un agente de programaci√≥n que puede trabajar **horas sin supervisi√≥n**, con acceso a internet, automatizaci√≥n de browser y gesti√≥n inteligente de memoria.

---

## Stack Tecnol√≥gico

```
VS Code + Claude Code + GLM-4.7-Flash + Ollama + Serper MCP + Puppeteer MCP + Ralph-Wiggum
```

| Componente | Funci√≥n | Ubicaci√≥n |
|------------|---------|-----------|
| **Claude Code** | Agente CLI de programaci√≥n | Container |
| **GLM-4.7-Flash** | LLM (82k contexto) | Host (Ollama) |
| **Ollama** | Servidor de inferencia | Host (nativo) |
| **Serper MCP** | B√∫squeda web (Google) | Container |
| **Puppeteer MCP** | Browser automation | Container |
| **Filesystem MCP** | Acceso a archivos | Container |
| **Ralph Wiggum** | Loops iterativos | Container |

---

## Caracter√≠sticas Principales

### ü§ñ LLM Local con Contexto Amplio
- Modelo GLM-4.7-Flash corriendo localmente via Ollama
- **82,000 tokens de contexto** (configurable hasta 198k)
- Aceleraci√≥n GPU con Apple Metal (macOS) o CUDA (Linux)
- Sin l√≠mites de API ni costos por token

### üåê B√∫squeda Web Integrada
- B√∫squeda en Google via Serper API
- B√∫squeda de noticias y im√°genes
- Web scraping con extracci√≥n de texto limpio
- El agente puede investigar antes de implementar

### üñ•Ô∏è Browser Automation
- Chromium headless integrado
- Navegaci√≥n, clicks, formularios
- Screenshots y evaluaci√≥n de JavaScript
- Ideal para testing y scraping de SPAs

### üìÅ Acceso Completo al Sistema de Archivos
- Lectura y escritura de archivos
- Navegaci√≥n de directorios
- El agente puede modificar tu c√≥digo directamente

### üîÑ Tareas de Larga Duraci√≥n (Ralph Wiggum)
- Loops iterativos con condiciones de salida
- Compresi√≥n autom√°tica de memoria al 70% del contexto
- Preservaci√≥n de trabajo en archivos y git
- Ideal para refactorings, migraciones, implementaciones complejas

### üì¶ Containerizado y Portable
- Todo empaquetado en Docker
- Configuraci√≥n persistente en volumen
- F√°cil de replicar en cualquier m√°quina

---

## Requisitos del Sistema

### Hardware Recomendado

| Componente | M√≠nimo | Recomendado |
|------------|--------|-------------|
| **RAM** | 32GB | 64GB+ |
| **VRAM/Unified** | 32GB | 64GB+ |
| **Disco** | 50GB libres | 100GB libres |
| **GPU** | Apple M1/M2 | Apple M3/M4 Max |

> **Nota sobre memoria**: GLM-4.7-Flash con 82k de contexto usa ~97GB de memoria (modelo q8_0 + KV cache). En Macs con memoria unificada, esto funciona bien con 128GB.

### Software Requerido

- **macOS** con Apple Silicon (M1/M2/M3/M4) para aceleraci√≥n GPU
  - Tambi√©n funciona en Linux con CUDA
- **Docker Desktop** v4.0+
- **Ollama v0.14.3-rc3+** (requerido para GLM-4.7-flash)
  - ‚ö†Ô∏è La versi√≥n estable 0.14.2 NO soporta este modelo

### Opcional

- **Serper API Key** - Para b√∫squedas web ([serper.dev](https://serper.dev))
- **VS Code** - Para integraci√≥n con Claude Code extension

---

## Instalaci√≥n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/karimtouma/kartostack.git
cd kartostack
```

### 2. Hacer Scripts Ejecutables

```bash
chmod +x scripts/*.sh
chmod +x entrypoint.sh
chmod +x plugins/ralph-wiggum/hooks/*.sh
chmod +x plugins/ralph-wiggum/scripts/*.sh
```

### 3. Instalar Ollama RC + Modelo

El script detecta tu sistema operativo y arquitectura autom√°ticamente:

```bash
# Solo instalar Ollama RC
./scripts/install-ollama.sh

# Instalar Ollama RC + descargar modelo + crear variante 82k
./scripts/install-ollama.sh --with-model
```

**¬øQu√© hace el script?**
1. Detecta si es macOS o Linux
2. Descarga Ollama v0.14.3-rc3 desde GitHub releases
3. Instala en `/Applications` (macOS) o `/usr/local/bin` (Linux)
4. Opcionalmente descarga `glm-4.7-flash` (~19-32GB)
5. Crea la variante `glm-4.7-flash-82k` con contexto expandido

### 4. Configurar Entorno

```bash
cp .env.example .env
```

Edita `.env` con tu configuraci√≥n:

```bash
# Modelo a usar
CLAUDE_MODEL=glm-4.7-flash-82k

# Tu directorio de proyectos
WORKSPACE=/Users/tu-usuario/Projects

# API key de Serper (opcional, para b√∫squeda web)
SERPER_API_KEY=tu_api_key_aqui
```

### 5. Construir Imagen Docker

```bash
docker build -t kartostack .
```

### 6. Ejecutar

```bash
./scripts/run.sh
```

---

## Uso

### Modo Interactivo (Default)

```bash
./scripts/run.sh
```

Esto abre una sesi√≥n interactiva donde puedes chatear con el agente.

### Modo No Interactivo

Ejecutar una tarea espec√≠fica y obtener el resultado:

```bash
# Tarea simple
./scripts/run.sh --print -p "Crea un script hello world en Python"

# Tarea con contexto
./scripts/run.sh --print -p "Analiza el c√≥digo en src/ y genera un reporte de arquitectura"
```

### Tareas de Larga Duraci√≥n (Ralph Wiggum)

Para tareas complejas que requieren m√∫ltiples iteraciones:

```bash
./scripts/run.sh
```

Dentro del agente:

```
/ralph-loop "Implementa autenticaci√≥n JWT en el proyecto" --max-iterations 30 --completion-promise "AUTH COMPLETE"
```

**Par√°metros:**
- `--max-iterations N` - M√°ximo de iteraciones (default: 50)
- `--completion-promise "TEXT"` - Texto que se√±ala completado

**Para se√±alar que terminaste:**
```
<promise>AUTH COMPLETE</promise>
```

### Ejemplos de Prompts Efectivos

#### Investigaci√≥n + Implementaci√≥n
```
Usa web_search para encontrar la documentaci√≥n oficial de FastAPI sobre
dependency injection. Luego implementa un sistema de autenticaci√≥n
basado en esos patrones en src/auth/. Criterio de completitud:
todos los tests en tests/auth/ deben pasar.
```

#### Refactoring con Tests
```
Refactoriza el m√≥dulo src/database/ para usar el patr√≥n Repository.
Criterios:
- Mantener 100% de compatibilidad con la API existente
- Agregar tests unitarios para cada repository
- pnpm test debe pasar sin errores
- Genera REFACTOR_REPORT.md con los cambios realizados
```

#### Scraping con Browser
```
Usa puppeteer_navigate para ir a https://example.com/pricing.
Extrae la tabla de precios y gu√°rdala como JSON en data/pricing.json.
Toma un screenshot y gu√°rdalo en screenshots/pricing.png.
```

---

## Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         DOCKER CONTAINER                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ                      CLAUDE CODE CLI                          ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Agente de programaci√≥n aut√≥nomo                          ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Lee/escribe archivos, ejecuta comandos                   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Protocolo MCP para herramientas externas                 ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Gesti√≥n de contexto y memoria                            ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                              ‚îÇ                                       ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ              ‚îÇ               ‚îÇ               ‚îÇ                      ‚îÇ
‚îÇ              ‚ñº               ‚ñº               ‚ñº                      ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ   ‚îÇ  SERPER MCP  ‚îÇ  ‚îÇFILESYSTEM MCP‚îÇ  ‚îÇPUPPETEER MCP ‚îÇ             ‚îÇ
‚îÇ   ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ             ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ web_search ‚îÇ  ‚îÇ ‚Ä¢ read_file  ‚îÇ  ‚îÇ ‚Ä¢ navigate   ‚îÇ             ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ news_search‚îÇ  ‚îÇ ‚Ä¢ write_file ‚îÇ  ‚îÇ ‚Ä¢ click      ‚îÇ             ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ scrape_url ‚îÇ  ‚îÇ ‚Ä¢ list_dir   ‚îÇ  ‚îÇ ‚Ä¢ fill       ‚îÇ             ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ fetch_url  ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ screenshot ‚îÇ             ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ                    RALPH WIGGUM PLUGIN                        ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ /ralph-loop - Iniciar loop iterativo                     ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ /cancel-ralph - Cancelar loop activo                     ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Auto-compresi√≥n de memoria al 70% del contexto           ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Preservaci√≥n de estado en archivos                       ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                                   ‚îÇ HTTP (Anthropic API)
                                   ‚îÇ localhost:11434
                                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         HOST (macOS/Linux)                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ                         OLLAMA                                ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Servidor de inferencia local                             ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ API compatible con Anthropic Messages                    ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   ‚Ä¢ Aceleraci√≥n GPU (Metal/CUDA)                             ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   Modelo: GLM-4.7-Flash                                      ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   Contexto: 82,000 tokens                                    ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ   Memoria: ~97GB (q8_0 + KV cache)                           ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                                                                ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### ¬øPor qu√© Ollama corre en el Host y no en Docker?

**Docker en macOS NO puede acceder a la GPU Metal**. Por lo tanto:
- Ollama debe correr **nativo** para usar aceleraci√≥n GPU (5-6x m√°s r√°pido)
- El container se conecta al host via `host.docker.internal:11434`
- Esto es transparente para el usuario

---

## Configuraci√≥n Avanzada

### Variables de Entorno

| Variable | Default | Descripci√≥n |
|----------|---------|-------------|
| `CLAUDE_MODEL` | `glm-4.7-flash-82k` | Modelo de Ollama a usar |
| `OLLAMA_HOST` | `localhost` | Host del servidor Ollama |
| `OLLAMA_PORT` | `11434` | Puerto del servidor Ollama |
| `WORKSPACE` | `$HOME/Projects` | Directorio montado como /workspace |
| `IMAGE_NAME` | `kartostack` | Nombre de la imagen Docker |
| `VOLUME_NAME` | `kartostack-data` | Nombre del volumen para persistencia |
| `SERPER_API_KEY` | - | API key de Serper para b√∫squeda web |

### Variantes del Modelo

El modelo base `glm-4.7-flash` tiene 198k de contexto m√°ximo. Puedes crear variantes con diferentes tama√±os:

```bash
# Variante r√°pida (32k contexto, menos memoria)
cat > /tmp/Modelfile.32k << 'EOF'
FROM glm-4.7-flash
PARAMETER num_ctx 32768
EOF
ollama create glm-4.7-flash-32k -f /tmp/Modelfile.32k

# Variante m√°xima (198k contexto, mucha memoria)
cat > /tmp/Modelfile.198k << 'EOF'
FROM glm-4.7-flash
PARAMETER num_ctx 198000
EOF
ollama create glm-4.7-flash-198k -f /tmp/Modelfile.198k
```

| Variante | Contexto | Memoria Aprox. | Uso |
|----------|----------|----------------|-----|
| 32k | 32,768 | ~40GB | Tareas r√°pidas |
| 82k | 82,000 | ~97GB | **Default** - Balance |
| 198k | 198,000 | ~200GB | Tareas con mucho contexto |

### Pre-carga Autom√°tica del Modelo (Anti Cold Start)

El script `run.sh` **pre-carga autom√°ticamente** el modelo en memoria antes de iniciar el container, eliminando el "cold start":

```bash
./scripts/run.sh
```

**Si el modelo ya est√° en memoria:**
```
‚úì Modelo glm-4.7-flash-82k ya est√° en memoria
```

**Si necesita cargarse (~1-2 minutos):**
```
üî• Pre-cargando modelo glm-4.7-flash-82k en memoria...
   (Esto puede tardar 1-2 minutos la primera vez)
   Cargando........................ ‚úì
```

El agente est√° listo para responder **inmediatamente** cuando se inicia la sesi√≥n.

### Mantener Modelo en Memoria 24/7

Para que Ollama **nunca** descargue el modelo de memoria (ideal para uso continuo):

```bash
# Opci√≥n 1: Variable de entorno al iniciar Ollama
OLLAMA_KEEP_ALIVE=-1 ollama serve

# Opci√≥n 2: Configurar al cargar el modelo
ollama run glm-4.7-flash-82k --keepalive -1
```

O agregar permanentemente en el Modelfile:
```
FROM glm-4.7-flash
PARAMETER num_ctx 82000
PARAMETER keep_alive -1
```

> **Nota**: Con `keep_alive=-1`, el modelo permanece en memoria (~97GB) hasta que reinicies Ollama o tu m√°quina.

### Configuraci√≥n de MCP Servers

Los servidores MCP est√°n configurados en `mcp-config.json`:

```json
{
  "mcpServers": {
    "serper": {
      "command": "node",
      "args": ["/app/mcp-servers/serper/index.js"],
      "env": { "SERPER_API_KEY": "${SERPER_API_KEY}" }
    },
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/workspace"]
    },
    "puppeteer": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-puppeteer"],
      "env": { "PUPPETEER_EXECUTABLE_PATH": "/usr/bin/chromium" }
    }
  }
}
```

---

## Herramientas MCP Disponibles

### Serper (B√∫squeda Web)

| Herramienta | Descripci√≥n | Ejemplo |
|-------------|-------------|---------|
| `web_search` | B√∫squeda en Google | `web_search("FastAPI authentication")` |
| `news_search` | B√∫squeda de noticias | `news_search("OpenAI GPT-5")` |
| `image_search` | B√∫squeda de im√°genes | `image_search("React architecture diagram")` |
| `scrape_webpage` | Scraping completo | `scrape_webpage("https://docs.python.org")` |
| `fetch_url` | Extracci√≥n de texto limpio | `fetch_url("https://example.com/article")` |

### Puppeteer (Browser Automation)

| Herramienta | Descripci√≥n |
|-------------|-------------|
| `puppeteer_navigate` | Navegar a URL |
| `puppeteer_screenshot` | Capturar screenshot |
| `puppeteer_click` | Click en elemento |
| `puppeteer_fill` | Llenar input/textarea |
| `puppeteer_select` | Seleccionar en dropdown |
| `puppeteer_hover` | Hover sobre elemento |
| `puppeteer_evaluate` | Ejecutar JavaScript |

### Filesystem

| Herramienta | Descripci√≥n |
|-------------|-------------|
| `read_file` | Leer contenido de archivo |
| `write_file` | Escribir/crear archivo |
| `list_directory` | Listar contenido de directorio |

---

## Plugin Ralph Wiggum

Ralph Wiggum implementa la t√©cnica de **loops iterativos con condiciones de salida** para tareas de larga duraci√≥n.

### Comandos Disponibles

| Comando | Descripci√≥n |
|---------|-------------|
| `/ralph-loop` | Iniciar loop iterativo |
| `/cancel-ralph` | Cancelar loop activo |
| `/help` | Ayuda del plugin |

### C√≥mo Funciona

1. Defines una tarea con criterios de completitud
2. El agente trabaja iterativamente
3. En cada iteraci√≥n, eval√∫a si cumpli√≥ los criterios
4. Si los cumple, imprime `<promise>CRITERIO</promise>`
5. El loop termina cuando se cumple o se alcanza max iterations

### Auto-Compresi√≥n de Memoria

Cuando el contexto alcanza ~70% del l√≠mite (70k tokens para modelo 82k):

1. El hook `stop-hook.sh` detecta el tama√±o
2. Guarda contexto cr√≠tico en `.claude/memory.md`
3. Inicia nueva sesi√≥n con instrucciones de leer el archivo
4. El agente contin√∫a donde qued√≥

Esto permite tareas que duran **horas** sin perder contexto.

---

## Soluci√≥n de Problemas

### Ollama no responde

```bash
# Verificar si Ollama est√° corriendo
curl http://localhost:11434/api/tags

# Ver logs de Ollama
tail -f ~/.ollama/logs/server.log

# Reiniciar Ollama
pkill ollama
open -a Ollama  # macOS
# o: systemctl restart ollama  # Linux
```

### Modelo no cargado en memoria

```bash
# Ver modelos cargados actualmente
ollama ps

# Pre-cargar el modelo
ollama run glm-4.7-flash-82k ""

# Verificar uso de memoria
# macOS:
vm_stat | grep "Pages"
# Linux:
free -h
```

### Error de permisos en Docker

```bash
# Resetear volumen (pierde configuraci√≥n persistida)
docker volume rm kartostack-data

# Reconstruir imagen
docker build --no-cache -t kartostack .
```

### Versi√≥n de Ollama incorrecta

```bash
# Verificar versi√≥n actual
ollama --version

# Si es menor a 0.14.3, reinstalar
./scripts/install-ollama.sh
```

### Serper MCP no funciona

```bash
# Verificar que la API key est√° configurada
echo $SERPER_API_KEY

# Probar la API directamente
curl -X POST 'https://google.serper.dev/search' \
  -H 'X-API-KEY: TU_API_KEY' \
  -H 'Content-Type: application/json' \
  -d '{"q": "test"}'
```

### Puppeteer no puede iniciar Chromium

```bash
# Verificar que Chromium est√° instalado en el container
docker run --rm kartostack which chromium

# Si falla, reconstruir imagen
docker build --no-cache -t kartostack .
```

### El agente se queda "pensando" mucho tiempo

- GLM-4.7-Flash es m√°s lento que modelos cloud
- En Apple Silicon M4 Max: ~60 tokens/segundo
- Prompts largos pueden tardar minutos
- Considera usar la variante 32k para tareas r√°pidas

---

## Estructura del Proyecto

```
kartostack/
‚îú‚îÄ‚îÄ .env.example              # Template de configuraci√≥n
‚îú‚îÄ‚îÄ .gitignore                # Archivos ignorados por git
‚îú‚îÄ‚îÄ Dockerfile                # Imagen del container
‚îú‚îÄ‚îÄ entrypoint.sh             # Script de entrada del container
‚îú‚îÄ‚îÄ Modelfile.82k             # Definici√≥n del modelo con 82k contexto
‚îú‚îÄ‚îÄ README.md                 # Esta documentaci√≥n
‚îú‚îÄ‚îÄ CLAUDE.md                 # Instrucciones para el modelo
‚îú‚îÄ‚îÄ claude.json               # Estado pre-configurado de Claude Code
‚îú‚îÄ‚îÄ mcp-config.json           # Configuraci√≥n de servidores MCP
‚îú‚îÄ‚îÄ settings.json             # Configuraci√≥n de Claude Code
‚îÇ
‚îú‚îÄ‚îÄ mcp-servers/
‚îÇ   ‚îî‚îÄ‚îÄ serper/
‚îÇ       ‚îú‚îÄ‚îÄ index.js          # Servidor MCP de Serper
‚îÇ       ‚îî‚îÄ‚îÄ package.json      # Dependencias
‚îÇ
‚îú‚îÄ‚îÄ plugins/
‚îÇ   ‚îî‚îÄ‚îÄ ralph-wiggum/
‚îÇ       ‚îú‚îÄ‚îÄ .claude-plugin/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ plugin.json   # Metadata del plugin
‚îÇ       ‚îú‚îÄ‚îÄ commands/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ralph-loop.md # Comando principal
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ cancel-ralph.md
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ compress-memory.md
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ help.md
‚îÇ       ‚îú‚îÄ‚îÄ hooks/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ hooks.json    # Configuraci√≥n de hooks
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ stop-hook.sh  # Hook de compresi√≥n
‚îÇ       ‚îú‚îÄ‚îÄ scripts/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ setup-ralph-loop.sh
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ compress-memory.sh
‚îÇ       ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ install-ollama.sh     # Instalador de Ollama RC
    ‚îî‚îÄ‚îÄ run.sh                # Script principal de ejecuci√≥n
```

---

## Seguridad

### Consideraciones

- El agente tiene **acceso completo** al directorio montado como workspace
- Puede ejecutar comandos de sistema dentro del container
- Puede hacer requests HTTP a internet (si Serper est√° configurado)
- **No lo uses en directorios con informaci√≥n sensible**

### Recomendaciones

1. **Monta solo lo necesario** - No montes `/` o `$HOME` completo
2. **Revisa los cambios** - Usa git para trackear cambios del agente
3. **Limita las iteraciones** - Siempre usa `--max-iterations` con Ralph
4. **Revisa el output** - Especialmente en tareas de larga duraci√≥n

---

## Contribuir

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

---

## Referencias

### Documentaci√≥n Oficial
- [Claude Code Docs](https://code.claude.com/docs/en/overview)
- [Ollama Documentation](https://ollama.com/docs)
- [MCP Protocol Specification](https://spec.modelcontextprotocol.io/)

### Art√≠culos y Gu√≠as
- [Agente de Coding 24/7 pr√°cticamente gratis](https://karim.touma.io/agente-de-ingenier%C3%ADa-de-software-24-7-pr%C3%A1cticamente-gratis-4cbd12e2bd92) - Gu√≠a completa del autor
- [Ollama + Claude Code](https://ollama.com/blog/claude) - Blog oficial de Ollama
- [Historia de Ralph Wiggum](https://www.humanlayer.dev/blog/brief-history-of-ralph) - Origen de la t√©cnica

### Recursos
- [GLM-4.7-Flash en Ollama](https://ollama.com/library/glm-4.7-flash)
- [Serper API](https://serper.dev)
- [Puppeteer MCP Server](https://github.com/anthropics/anthropic-quickstarts)

---

## Autor

**Karim Touma**

- [Blog](https://karim.touma.io)
- [LinkedIn](https://www.linkedin.com/in/katouma/)
- [Twitter/X](https://x.com/karim_op)

---

## Licencia

MIT License - ver [LICENSE](LICENSE) para detalles.

---

<p align="center">
  <i>Hecho con ‚ù§Ô∏è y mucho caf√© por <a href="https://karim.touma.io">Karim Touma</a></i>
</p>
